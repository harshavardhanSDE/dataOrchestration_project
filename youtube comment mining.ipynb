{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from getComments import get_comments\n",
        "from extract_data import extract_comments\n",
        "from mine import (\n",
        "    preprocess_text,\n",
        "    analyze_sentiment,\n",
        "    generate_wordcloud,\n",
        "    perform_clustering,\n",
        "    visualize_clusters,\n",
        "    visualize_term_frequencies\n",
        ")\n",
        "\n",
        "st.title(\"YouTube Comment Analyzer\")\n",
        "st.markdown(\"Enter a YouTube **Video ID** to fetch and analyze its comments.\")\n",
        "\n",
        "# Input field for YouTube video ID\n",
        "video_id = st.text_input(\"Enter Video ID\", \"\")\n",
        "\n",
        "if st.button(\"Fetch Comments\"):\n",
        "    if video_id:\n",
        "        try:\n",
        "            # Fetch comments using the YouTube API\n",
        "            df = get_comments(video_id)\n",
        "\n",
        "            if df is None or df.empty:  # Check for None and empty DataFrame\n",
        "                st.warning(\"No comments found for this video.\")\n",
        "            else:\n",
        "                st.success(f\"Fetched {len(df)} comments successfully!\")\n",
        "                st.dataframe(df)  # Display comments in a table\n",
        "\n",
        "                # Preprocess comments for text mining\n",
        "                df[\"cleaned_comment\"] = df[\"comment\"].apply(preprocess_text)\n",
        "                df[\"sentiment\"] = df[\"cleaned_comment\"].apply(analyze_sentiment)\n",
        "\n",
        "                # Display sentiment analysis\n",
        "                st.subheader(\"Sentiment Distribution\")\n",
        "                sentiment_counts = df[\"sentiment\"].value_counts()\n",
        "                st.bar_chart(sentiment_counts)\n",
        "\n",
        "                # Generate and display word cloud\n",
        "                st.subheader(\"Word Cloud\")\n",
        "                all_cleaned_text = \" \".join(df[\"cleaned_comment\"])\n",
        "                img = generate_wordcloud(all_cleaned_text)\n",
        "                st.image(img, caption='Word Cloud', use_column_width=True)\n",
        "\n",
        "                # Perform clustering\n",
        "                n_clusters = 3  # Set the number of clusters\n",
        "                try:\n",
        "                    labels, feature_names, tfidf_matrix = perform_clustering(df[\"cleaned_comment\"], n_clusters)\n",
        "\n",
        "                    # Visualize clusters\n",
        "                    st.subheader(\"Cluster Visualization\")\n",
        "                    visualize_clusters(labels, tfidf_matrix, n_clusters)\n",
        "\n",
        "                    # Calculate and visualize term frequencies\n",
        "                    term_frequencies = df[\"cleaned_comment\"].str.split(expand=True).stack().value_counts()\n",
        "                    st.subheader(\"Term Frequencies Visualization\")\n",
        "                    visualize_term_frequencies(term_frequencies)\n",
        "                except Exception as e:\n",
        "                    st.error(f\"An error occurred during clustering: {e}\")\n",
        "\n",
        "                # Provide download button for processed data\n",
        "                st.download_button(\n",
        "                    label=\"Download Processed Data\",\n",
        "                    data=df.to_csv(index=False),\n",
        "                    file_name=\"processed_comments.csv\",\n",
        "                    mime=\"text/csv\"\n",
        "                )\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred while fetching comments: {e}\")\n",
        "    else:\n",
        "        st.error(\"Please enter a valid YouTube Video ID.\")\n"
      ],
      "metadata": {
        "id": "aNSyslqXclsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def extract_comments():\n",
        "    \"\"\"Extracts comments from a JSON file into a DataFrame.\"\"\"\n",
        "    with open(\"data.json\", \"r\") as json_file:\n",
        "        data = json.load(json_file)\n",
        "\n",
        "    comments = []\n",
        "    for item in data.get(\"items\", []):\n",
        "        comment_snippet = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
        "        comment_data = {\n",
        "            \"author\": comment_snippet[\"authorDisplayName\"],\n",
        "            \"author_channel_url\": comment_snippet[\"authorChannelUrl\"],\n",
        "            \"comment\": comment_snippet[\"textOriginal\"],\n",
        "            \"like_count\": comment_snippet[\"likeCount\"],\n",
        "            \"published_at\": comment_snippet[\"publishedAt\"],\n",
        "            \"updated_at\": comment_snippet[\"updatedAt\"]\n",
        "        }\n",
        "        comments.append(comment_data)\n",
        "\n",
        "    return pd.DataFrame(comments)\n"
      ],
      "metadata": {
        "id": "XFWkYy_Vc2fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "\n",
        "def get_comments(video_id):\n",
        "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
        "    api_service_name = \"youtube\"\n",
        "    api_version = \"v3\"\n",
        "    DEVELOPER_KEY = \"AIzaSyDzhV8FCEfGG0auMqA-ibZq0GfYgKIs3j8\"  # Replace with your actual API key\n",
        "\n",
        "    youtube = googleapiclient.discovery.build(api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
        "\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet,replies\",\n",
        "        videoId=video_id,\n",
        "        maxResults=100  # Adjust as necessary\n",
        "    )\n",
        "    try:\n",
        "        response = request.execute()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching comments: {e}\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame on error\n",
        "\n",
        "    # Extract comments and return as DataFrame\n",
        "    comments = []\n",
        "    for item in response.get(\"items\", []):\n",
        "        comment_snippet = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
        "        comments.append({\n",
        "            \"author\": comment_snippet[\"authorDisplayName\"],\n",
        "            \"author_channel_url\": comment_snippet[\"authorChannelUrl\"],\n",
        "            \"comment\": comment_snippet[\"textOriginal\"],\n",
        "            \"like_count\": comment_snippet[\"likeCount\"],\n",
        "            \"published_at\": comment_snippet[\"publishedAt\"],\n",
        "            \"updated_at\": comment_snippet[\"updatedAt\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(comments)\n"
      ],
      "metadata": {
        "id": "ATrqA6jnc9jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "import io\n",
        "import streamlit as st\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans the text for analysis.\"\"\"\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)  # Remove @ mentions and # hashtags\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.strip()  # Remove leading/trailing whitespaces\n",
        "    return text\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"Analyzes the sentiment of the given text.\"\"\"\n",
        "    analysis = TextBlob(text)\n",
        "    if analysis.sentiment.polarity > 0:\n",
        "        return 'Positive'\n",
        "    elif analysis.sentiment.polarity < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "def generate_wordcloud(text):\n",
        "    \"\"\"Generates a word cloud image from the given text.\"\"\"\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    img = io.BytesIO()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.savefig(img, format='png')\n",
        "    plt.close()  # Close the plot to avoid displaying it\n",
        "    img.seek(0)\n",
        "\n",
        "    return img\n",
        "\n",
        "def visualize_term_frequencies(term_frequencies):\n",
        "    \"\"\"Visualizes the term frequencies using Seaborn.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=term_frequencies.values, y=term_frequencies.index, palette='viridis', hue=term_frequencies.index)\n",
        "    plt.title(\"Term Frequencies\")\n",
        "    plt.xlabel(\"Frequency\")\n",
        "    plt.ylabel(\"Terms\")\n",
        "    plt.legend(title='Terms', loc='upper right', bbox_to_anchor=(1.2, 1))\n",
        "    st.pyplot(plt)  # Use Streamlit's pyplot to display the plot\n",
        "    plt.close()  # Close the plot to avoid displaying it again\n",
        "\n",
        "def perform_clustering(text_data, n_clusters):\n",
        "    \"\"\"Performs clustering on the cleaned text data.\"\"\"\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.cluster import KMeans\n",
        "\n",
        "    # Create the TF-IDF matrix\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(text_data)\n",
        "\n",
        "    # Perform KMeans clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(tfidf_matrix)\n",
        "\n",
        "    return kmeans.labels_, vectorizer.get_feature_names_out(), tfidf_matrix\n",
        "\n",
        "def visualize_clusters(labels, tfidf_matrix, n_clusters):\n",
        "    \"\"\"Visualizes the clusters using a scatter plot.\"\"\"\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    # Reduce dimensions using PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_data = pca.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
        "    plt.title(f\"K-Means Clustering (n_clusters={n_clusters})\")\n",
        "    plt.xlabel(\"PCA Component 1\")\n",
        "    plt.ylabel(\"PCA Component 2\")\n",
        "    plt.colorbar(label='Cluster Label')\n",
        "    st.pyplot(plt)  # Use Streamlit's pyplot to display the plot\n",
        "    plt.close()  # Close the plot to avoid displaying it again\n"
      ],
      "metadata": {
        "id": "CirGhu5HdHlp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}